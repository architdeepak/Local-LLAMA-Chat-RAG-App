# LLAMA2-ChatUI-CPU

These are the following instructions for running a local application to communicate with Llama 2 or other LLMs for CPU-based inference.

# Step 1: 
Open terminal and install nicegui.



Then install llama-cpp.




# Step 2: 
Navigate to [hugging face]([url](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/tree/main)) to download the [llama-2â€“7b-chat.Q2_K.gguf]([url](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/tree/main)) file: 
[https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/tree/main](url). Other model versions also work, as long as they are llama-cpp supported.

# Step 3: 
Clone this github repo into a folder. Move the downloaded Llama 2 model into the same folder.

# Step 4: 
Go to file explorer and open the designated folder. Open the mainpy.py file.

<img width="581" alt="image" src="https://github.com/user-attachments/assets/aa56f778-36e6-42b2-9415-6120105aa6d6">

# Step 5: 
Ensure that the model_path parameter is set to correct path to the model file (just the file name if all steps have been followed properly)



# Step 6: 
Open terminal and navigate to the designated folder. Then run python mainpy.py.



# Step 7: 
A website should pop up on your browser. Instead of being web-based, it can be used as an app.



# Step 8: 
Now enjoy chatting with Llama 2 or another LLM!

